{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND5swnI6pFHs",
        "outputId": "bfba738c-1b91-4796-c20d-2c85be23d7bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dashscope\n",
        "!pip install modelscope_studio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63IhggY_Qkg",
        "outputId": "b910c1da-f0e4-4df4-8436-52e30d09ec03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dashscope\n",
            "  Downloading dashscope-1.23.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from dashscope) (3.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dashscope) (2.32.3)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from dashscope) (1.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (2025.4.26)\n",
            "Downloading dashscope-1.23.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dashscope\n",
            "Successfully installed dashscope-1.23.2\n",
            "Collecting modelscope_studio\n",
            "  Downloading modelscope_studio-1.2.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: gradio<6.0,>=4.43.0 in /usr/local/lib/python3.11/dist-packages (from modelscope_studio) (5.29.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.11.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0,>=4.43.0->modelscope_studio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio<6.0,>=4.43.0->modelscope_studio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio<6.0,>=4.43.0->modelscope_studio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<6.0,>=4.43.0->modelscope_studio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<6.0,>=4.43.0->modelscope_studio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<6.0,>=4.43.0->modelscope_studio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<6.0,>=4.43.0->modelscope_studio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio<6.0,>=4.43.0->modelscope_studio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio<6.0,>=4.43.0->modelscope_studio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio<6.0,>=4.43.0->modelscope_studio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio<6.0,>=4.43.0->modelscope_studio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.43.0->modelscope_studio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.43.0->modelscope_studio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio<6.0,>=4.43.0->modelscope_studio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0,>=4.43.0->modelscope_studio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0,>=4.43.0->modelscope_studio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0,>=4.43.0->modelscope_studio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio<6.0,>=4.43.0->modelscope_studio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio<6.0,>=4.43.0->modelscope_studio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio<6.0,>=4.43.0->modelscope_studio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0,>=4.43.0->modelscope_studio) (0.1.2)\n",
            "Downloading modelscope_studio-1.2.4-py3-none-any.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: modelscope_studio\n",
            "Successfully installed modelscope_studio-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers from source - only needed for versions <= v4.34\n",
        "# pip install git+https://github.com/huggingface/transformers.git\n",
        "# pip install accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "# <|system|>\n",
        "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
        "# <|user|>\n",
        "# How many helicopters can a human eat in one sitting?</s>\n",
        "# <|assistant|>\n",
        "# ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fYC3PRT-3UDh",
        "outputId": "e10a92f8-87ef-4b0f-ab1e-4af03d4618ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-74bb851a6b1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# <|system|>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             )\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "system_message = {\"role\": \"system\", \"content\": (\"You are a Premium Chatbot who thinks before speaking and uses 2 paragraphs: one for Reasoning and another for Response. When replying, format your answer using the following tags exactly: output your reasoning with '<|assistant reasoning|>' followed by your chain-of-thought explanation, and then output your final reply with '<|assistant response|>' followed by your answer. Also, please use lowercase for '<|system|>' and '<|user|>' tags.\")}\n",
        "conversation = [system_message]\n",
        "def reformat_output(output_text: str) -> str:\n",
        "    reformatted = output_text.replace(\"<System>\", \"<|system|>\").replace(\"<User>\", \"<|user|>\")\n",
        "    if \"<assistant>\" in reformatted:\n",
        "        before, assistant_block = reformatted.split(\"<assistant>\", 1)\n",
        "        if \"Reasoning:\" in assistant_block and \"Answer:\" in assistant_block:\n",
        "            reasoning_part = assistant_block.split(\"Answer:\")[0]\n",
        "            response_part = assistant_block.split(\"Answer:\")[1]\n",
        "            reasoning_part = reasoning_part.replace(\"Reasoning:\", \"\").strip()\n",
        "            response_part = response_part.strip()\n",
        "            new_assistant_text = f\"<|assistant reasoning|> {reasoning_part}\\n<|assistant response|> {response_part}\"\n",
        "            reformatted = before + new_assistant_text\n",
        "        else:\n",
        "            reformatted = reformatted.replace(\"<assistant>\", \"<|assistant reasoning|>\")\n",
        "    return reformatted\n",
        "print(\"Type 'bye' to exit the chat.\")\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.strip().lower() == \"bye\":\n",
        "        print(\"Chatbot: Farewell, Sir!\")\n",
        "        break\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "    prompt = pipe.tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    start_time = time.time()\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    end_time = time.time()\n",
        "    response_time = end_time - start_time\n",
        "    generated_text = outputs[0][\"generated_text\"]\n",
        "    formatted_output = reformat_output(generated_text)\n",
        "    print(\"Chatbot:\")\n",
        "    print(formatted_output)\n",
        "    print(f\"\\nResponse Time: {response_time:.2f} seconds\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": generated_text})\n"
      ],
      "metadata": {
        "id": "wwY8xUnNJX0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "system_message = {\"role\": \"system\", \"content\": (\"You are a Premium Chatbot who thinks before speaking and uses 2 paragraphs: one for Reasoning and another for Response. When replying, format your answer using the following tags exactly: output your reasoning with '<|assistant reasoning|>' followed by your chain-of-thought explanation, and then output your final reply with '<|assistant response|>' followed by your answer. Also, please use lowercase for '<|system|>' and '<|user|>' tags.\")}\n",
        "conversation = [system_message]\n",
        "\n",
        "def reformat_output(output_text: str) -> str:\n",
        "    reformatted = output_text.replace(\"<System>\", \"<|system|>\").replace(\"<User>\", \"<|user|>\")\n",
        "    if \"<assistant>\" in reformatted:\n",
        "        before, assistant_block = reformatted.split(\"<assistant>\", 1)\n",
        "        if \"Reasoning:\" in assistant_block and \"Answer:\" in assistant_block:\n",
        "            reasoning_part = assistant_block.split(\"Answer:\")[0]\n",
        "            response_part = assistant_block.split(\"Answer:\")[1]\n",
        "            reasoning_part = reasoning_part.replace(\"Reasoning:\", \"\").strip()\n",
        "            response_part = response_part.strip()\n",
        "            new_assistant_text = f\"<|assistant reasoning|> {reasoning_part}\\n<|assistant response|> {response_part}\"\n",
        "            reformatted = before + new_assistant_text\n",
        "        else:\n",
        "            reformatted = reformatted.replace(\"<assistant>\", \"<|assistant reasoning|>\")\n",
        "    return reformatted\n",
        "\n",
        "def chat(user_input, state):\n",
        "    if user_input.strip().lower() == \"bye\":\n",
        "        return \"Chatbot: Farewell, Sir!\", state\n",
        "    state.append({\"role\": \"user\", \"content\": user_input})\n",
        "    prompt = pipe.tokenizer.apply_chat_template(state, tokenize=False, add_generation_prompt=True)\n",
        "    start_time = time.time()\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    end_time = time.time()\n",
        "    response_time = end_time - start_time\n",
        "    generated_text = outputs[0][\"generated_text\"]\n",
        "    formatted_output = reformat_output(generated_text)\n",
        "    state.append({\"role\": \"assistant\", \"content\": generated_text})\n",
        "    result = f\"{formatted_output}\\n\\nResponse Time: {response_time:.2f} seconds\"\n",
        "    return result, state\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## LogicLink Chatbot\")\n",
        "    chatbot_state = gr.State(value=conversation)\n",
        "    chatbot_output = gr.Textbox(label=\"Chatbot Response\", interactive=False)\n",
        "    user_input_text = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\")\n",
        "    send_btn = gr.Button(\"Send\")\n",
        "    send_btn.click(chat, inputs=[user_input_text, chatbot_state], outputs=[chatbot_output, chatbot_state])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "xF022FS2myho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_formatted_output(user_prompt):\n",
        "    system_text = (\n",
        "        \"You are a Premium Chatbot who thinks before speaking and always responds \"\n",
        "        \"in the specified format.\"\n",
        "    )\n",
        "\n",
        "    # Reasoning is based on the provided reasoning prompt\n",
        "    assistant_reasoning = \"Different ways to approach the Query : \" + user_prompt + \" [Insert chain-of-thought reasoning here.]\"\n",
        "\n",
        "    # Response is based on the provided response prompt (echoing the query here)\n",
        "    assistant_response = user_prompt  # Replace with your actual response logic if needed\n",
        "\n",
        "    formatted_output = (\n",
        "        f\"<|system|>\\n{system_text}\\n\"\n",
        "        f\"<|user|>\\n{user_prompt}\\n\"\n",
        "        f\"<|assistant reasoning|>\\n{assistant_reasoning}\\n\"\n",
        "        f\"<|assistant response|>\\n{assistant_response}\"\n",
        "    )\n",
        "\n",
        "    return formatted_output\n",
        "\n",
        "# Example usage:\n",
        "user_input = \"hello how are you\"\n",
        "output = generate_formatted_output(user_input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "Iu4_2VMtr6sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "def logiclink_chat(user_input, history):\n",
        "    if not user_input:\n",
        "        return history, \"\"\n",
        "    # Append user message\n",
        "    history.append((\"You\", user_input))\n",
        "\n",
        "    # Build prompts for reasoning and response\n",
        "    reasoning_prompt = (\n",
        "        \"Reflect on all potential nuances, underlying concepts, and alternative perspectives \"\n",
        "        \"before forming a final answer for the following query: \" + user_input\n",
        "    )\n",
        "    response_prompt = user_input\n",
        "\n",
        "    start = time.time()\n",
        "    # Generate chain-of-thought reasoning\n",
        "    reasoning_out = pipe(\n",
        "        reasoning_prompt,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    reasoning = reasoning_out[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Generate final answer\n",
        "    response_out = pipe(\n",
        "        response_prompt,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    response = response_out[0][\"generated_text\"].strip()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    # Append reasoning and response to history\n",
        "    history.append((\"LogicLink – Reasoning\", reasoning))\n",
        "    history.append((\"LogicLink - Response\", f\"{response}\\n\\n*({elapsed:.2f}s)*\"))\n",
        "\n",
        "    return history, \"\"\n",
        "\n",
        "# Build Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## LogicLink Chat\")\n",
        "            chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
        "            user_input = gr.Textbox(show_label=False, placeholder=\"Type your query here…\")\n",
        "            send = gr.Button(\"Send\", variant=\"primary\")\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\n",
        "                \"**How LogicLink Works**\\n\\n\"\n",
        "                \"- You type a question.\\n\"\n",
        "                \"- LogicLink first shows its _chain-of-thought_.\\n\"\n",
        "                \"- Then it gives the concise answer below.\\n\"\n",
        "                \"- Conversation history is on the left.\"\n",
        "            )\n",
        "\n",
        "    send.click(logiclink_chat, [user_input, chatbot], [chatbot, user_input])\n",
        "    user_input.submit(logiclink_chat, [user_input, chatbot], [chatbot, user_input])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "_SM-lgi0s_VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U transformers peft"
      ],
      "metadata": {
        "id": "20xJ4VLf3DAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "def logiclink_chat(user_input, history):\n",
        "    if not user_input:\n",
        "        return history, \"\"\n",
        "    # Append user message\n",
        "    history.append((\"You\", user_input))\n",
        "\n",
        "    # Build reasoning prompt\n",
        "    reasoning_prompt = (\n",
        "        \"Reflect on all potential nuances, underlying concepts, and alternative perspectives \"\n",
        "        \"before forming a final answer for the following query: \" + user_input\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    # Generate chain-of-thought reasoning\n",
        "    reasoning = pipe(\n",
        "        reasoning_prompt,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Generate final answer\n",
        "    response = pipe(\n",
        "        user_input,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )[0][\"generated_text\"].strip()\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    # Append reasoning and response\n",
        "    history.append((\"LogicLink – Reasoning\", reasoning))\n",
        "    history.append((\"LogicLink – Response\", f\"{response}\\n\\n*({elapsed:.2f}s)*\"))\n",
        "\n",
        "    return history, \"\"\n",
        "\n",
        "# Custom CSS for black background, red accents, blue borders\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "    background-color: #000;\n",
        "    color: #fff;\n",
        "}\n",
        ".gradio-container {\n",
        "    border-radius: 8px;\n",
        "    padding: 1rem;\n",
        "}\n",
        ".gr-button, .gr-button:hover {\n",
        "    background-color: #e53935 !important;  /* red buttons */\n",
        "    color: #fff !important;\n",
        "}\n",
        ".gr-textbox, .gr-chatbot {\n",
        "    border: 2px solid #1e88e5 !important;  /* blue borders */\n",
        "}\n",
        ".gr-textbox textarea, .gr-chatbot {\n",
        "    background-color: #111;\n",
        "    color: #fff;\n",
        "}\n",
        ".gr-row {\n",
        "    margin-bottom: 1rem;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## 🎨 LogicLink Chatbot\")\n",
        "            chatbot = gr.Chatbot(label=\"Conversation\", height=600)\n",
        "            user_input = gr.Textbox(show_label=False, placeholder=\"Type your query here…\")\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\n",
        "                \"**How LogicLink Works**\\n\\n\"\n",
        "                \"- You type a question.\\n\"\n",
        "                \"- LogicLink shows its _chain-of-thought_ first.\\n\"\n",
        "                \"- Then it gives the concise answer below.\\n\"\n",
        "                \"- Chat history stays on the left.\"\n",
        "            )\n",
        "\n",
        "    send_btn.click(logiclink_chat, [user_input, chatbot], [chatbot, user_input])\n",
        "    user_input.submit(logiclink_chat, [user_input, chatbot], [chatbot, user_input])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "cTwpztAWy2r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import modelscope_studio.components.antd as antd\n",
        "import modelscope_studio.components.antdx as antdx\n",
        "import modelscope_studio.components.base as ms\n",
        "import modelscope_studio.components.pro as pro\n",
        "import dashscope\n",
        "from config import DEFAULT_LOCALE, DEFAULT_SETTINGS, DEFAULT_THEME, DEFAULT_SUGGESTIONS, save_history, get_text, user_config, bot_config, welcome_config, api_key, MODEL_OPTIONS_MAP\n",
        "from ui_components.logo import Logo\n",
        "from ui_components.settings_header import SettingsHeader\n",
        "from ui_components.thinking_button import ThinkingButton\n",
        "from dashscope import Generation\n",
        "\n",
        "dashscope.api_key = api_key\n",
        "\n",
        "\n",
        "def format_history(history, sys_prompt):\n",
        "    # messages = [{\n",
        "    #     \"role\": \"system\",\n",
        "    #     \"content\": sys_prompt,\n",
        "    # }]\n",
        "    messages = []\n",
        "    for item in history:\n",
        "        if item[\"role\"] == \"user\":\n",
        "            messages.append({\"role\": \"user\", \"content\": item[\"content\"]})\n",
        "        elif item[\"role\"] == \"assistant\":\n",
        "            contents = [{\n",
        "                \"type\": \"text\",\n",
        "                \"text\": content[\"content\"]\n",
        "            } for content in item[\"content\"] if content[\"type\"] == \"text\"]\n",
        "            messages.append({\n",
        "                \"role\":\n",
        "                \"assistant\",\n",
        "                \"content\":\n",
        "                contents[0][\"text\"] if len(contents) > 0 else \"\"\n",
        "            })\n",
        "    return messages\n",
        "\n",
        "\n",
        "class Gradio_Events:\n",
        "\n",
        "    @staticmethod\n",
        "    def submit(state_value):\n",
        "\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        settings = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"settings\"]\n",
        "        enable_thinking = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"enable_thinking\"]\n",
        "        model = settings.get(\"model\")\n",
        "        messages = format_history(history,\n",
        "                                  sys_prompt=settings.get(\"sys_prompt\", \"\"))\n",
        "\n",
        "        history.append({\n",
        "            \"role\":\n",
        "            \"assistant\",\n",
        "            \"content\": [],\n",
        "            \"key\":\n",
        "            str(uuid.uuid4()),\n",
        "            \"header\":\n",
        "            MODEL_OPTIONS_MAP.get(model, {}).get(\"label\", None),\n",
        "            \"loading\":\n",
        "            True,\n",
        "            \"status\":\n",
        "            \"pending\"\n",
        "        })\n",
        "\n",
        "        yield {\n",
        "            chatbot: gr.update(value=history),\n",
        "            state: gr.update(value=state_value),\n",
        "        }\n",
        "        try:\n",
        "            response = Generation.call(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                stream=True,\n",
        "                result_format='message',\n",
        "                incremental_output=True,\n",
        "                enable_thinking=enable_thinking,\n",
        "                thinking_budget=settings.get(\"thinking_budget\", 1) * 1024)\n",
        "            start_time = time.time()\n",
        "            reasoning_content = \"\"\n",
        "            answer_content = \"\"\n",
        "            is_thinking = False\n",
        "            is_answering = False\n",
        "            contents = [None, None]\n",
        "            for chunk in response:\n",
        "                if (not chunk.output.choices[0].message.get(\"content\")\n",
        "                        and not chunk.output.choices[0].message.get(\n",
        "                            \"reasoning_content\")):\n",
        "                    pass\n",
        "                else:\n",
        "                    delta = chunk.output.choices[0].message\n",
        "                    if hasattr(\n",
        "                            delta,\n",
        "                            'reasoning_content') and delta.reasoning_content:\n",
        "                        if not is_thinking:\n",
        "                            contents[0] = {\n",
        "                                \"type\": \"tool\",\n",
        "                                \"content\": \"\",\n",
        "                                \"options\": {\n",
        "                                    \"title\": get_text(\"Thinking...\", \"思考中...\"),\n",
        "                                    \"status\": \"pending\"\n",
        "                                },\n",
        "                                \"copyable\": False,\n",
        "                                \"editable\": False\n",
        "                            }\n",
        "                            is_thinking = True\n",
        "                        reasoning_content += delta.reasoning_content\n",
        "                    if hasattr(delta, 'content') and delta.content:\n",
        "                        if not is_answering:\n",
        "                            thought_cost_time = \"{:.2f}\".format(time.time() -\n",
        "                                                                start_time)\n",
        "                            if contents[0]:\n",
        "                                contents[0][\"options\"][\"title\"] = get_text(\n",
        "                                    f\"End of Thought ({thought_cost_time}s)\",\n",
        "                                    f\"已深度思考 (用时{thought_cost_time}s)\")\n",
        "                                contents[0][\"options\"][\"status\"] = \"done\"\n",
        "                            contents[1] = {\n",
        "                                \"type\": \"text\",\n",
        "                                \"content\": \"\",\n",
        "                            }\n",
        "\n",
        "                            is_answering = True\n",
        "                        answer_content += delta.content\n",
        "\n",
        "                    if contents[0]:\n",
        "                        contents[0][\"content\"] = reasoning_content\n",
        "                    if contents[1]:\n",
        "                        contents[1][\"content\"] = answer_content\n",
        "                history[-1][\"content\"] = [\n",
        "                    content for content in contents if content\n",
        "                ]\n",
        "\n",
        "                history[-1][\"loading\"] = False\n",
        "                yield {\n",
        "                    chatbot: gr.update(value=history),\n",
        "                    state: gr.update(value=state_value)\n",
        "                }\n",
        "            print(\"model: \", model, \"-\", \"reasoning_content: \",\n",
        "                  reasoning_content, \"\\n\", \"content: \", answer_content)\n",
        "            history[-1][\"status\"] = \"done\"\n",
        "            cost_time = \"{:.2f}\".format(time.time() - start_time)\n",
        "            history[-1][\"footer\"] = get_text(f\"{cost_time}s\",\n",
        "                                             f\"用时{cost_time}s\")\n",
        "            yield {\n",
        "                chatbot: gr.update(value=history),\n",
        "                state: gr.update(value=state_value),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(\"model: \", model, \"-\", \"Error: \", e)\n",
        "            history[-1][\"loading\"] = False\n",
        "            history[-1][\"status\"] = \"done\"\n",
        "            history[-1][\"content\"] += [{\n",
        "                \"type\":\n",
        "                \"text\",\n",
        "                \"content\":\n",
        "                f'<span style=\"color: var(--color-red-500)\">{str(e)}</span>'\n",
        "            }]\n",
        "            yield {\n",
        "                chatbot: gr.update(value=history),\n",
        "                state: gr.update(value=state_value)\n",
        "            }\n",
        "            raise e\n",
        "\n",
        "    @staticmethod\n",
        "    def add_message(input_value, settings_form_value, thinking_btn_state_value,\n",
        "                    state_value):\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            random_id = str(uuid.uuid4())\n",
        "            history = []\n",
        "            state_value[\"conversation_id\"] = random_id\n",
        "            state_value[\"conversation_contexts\"][\n",
        "                state_value[\"conversation_id\"]] = {\n",
        "                    \"history\": history\n",
        "                }\n",
        "            state_value[\"conversations\"].append({\n",
        "                \"label\": input_value,\n",
        "                \"key\": random_id\n",
        "            })\n",
        "\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "\n",
        "        state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]] = {\n",
        "                \"history\": history,\n",
        "                \"settings\": settings_form_value,\n",
        "                \"enable_thinking\": thinking_btn_state_value[\"enable_thinking\"]\n",
        "            }\n",
        "        history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": input_value,\n",
        "            \"key\": str(uuid.uuid4())\n",
        "        })\n",
        "        yield Gradio_Events.preprocess_submit(clear_input=True)(state_value)\n",
        "\n",
        "        try:\n",
        "            for chunk in Gradio_Events.submit(state_value):\n",
        "                yield chunk\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            yield Gradio_Events.postprocess_submit(state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_submit(clear_input=True):\n",
        "\n",
        "        def preprocess_submit_handler(state_value):\n",
        "            history = state_value[\"conversation_contexts\"][\n",
        "                state_value[\"conversation_id\"]][\"history\"]\n",
        "            return {\n",
        "                **({\n",
        "                    input:\n",
        "                    gr.update(value=None, loading=True) if clear_input else gr.update(loading=True),\n",
        "                } if clear_input else {}),\n",
        "                conversations:\n",
        "                gr.update(active_key=state_value[\"conversation_id\"],\n",
        "                          items=list(\n",
        "                              map(\n",
        "                                  lambda item: {\n",
        "                                      **item,\n",
        "                                      \"disabled\":\n",
        "                                      True if item[\"key\"] != state_value[\n",
        "                                          \"conversation_id\"] else False,\n",
        "                                  }, state_value[\"conversations\"]))),\n",
        "                add_conversation_btn:\n",
        "                gr.update(disabled=True),\n",
        "                clear_btn:\n",
        "                gr.update(disabled=True),\n",
        "                conversation_delete_menu_item:\n",
        "                gr.update(disabled=True),\n",
        "                chatbot:\n",
        "                gr.update(value=history,\n",
        "                          bot_config=bot_config(\n",
        "                              disabled_actions=['edit', 'retry', 'delete']),\n",
        "                          user_config=user_config(\n",
        "                              disabled_actions=['edit', 'delete'])),\n",
        "                state:\n",
        "                gr.update(value=state_value),\n",
        "            }\n",
        "\n",
        "        return preprocess_submit_handler\n",
        "\n",
        "    @staticmethod\n",
        "    def postprocess_submit(state_value):\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        return {\n",
        "            input:\n",
        "            gr.update(loading=False),\n",
        "            conversation_delete_menu_item:\n",
        "            gr.update(disabled=False),\n",
        "            clear_btn:\n",
        "            gr.update(disabled=False),\n",
        "            conversations:\n",
        "            gr.update(items=state_value[\"conversations\"]),\n",
        "            add_conversation_btn:\n",
        "            gr.update(disabled=False),\n",
        "            chatbot:\n",
        "            gr.update(value=history,\n",
        "                      bot_config=bot_config(),\n",
        "                      user_config=user_config()),\n",
        "            state:\n",
        "            gr.update(value=state_value),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def cancel(state_value):\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        history[-1][\"loading\"] = False\n",
        "        history[-1][\"status\"] = \"done\"\n",
        "        history[-1][\"footer\"] = get_text(\"Chat completion paused\", \"对话已暂停\")\n",
        "        return Gradio_Events.postprocess_submit(state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def delete_message(state_value, e: gr.EventData):\n",
        "        index = e._data[\"payload\"][0][\"index\"]\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        history = history[:index] + history[index + 1:]\n",
        "\n",
        "        state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"] = history\n",
        "\n",
        "        return gr.update(value=state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def edit_message(state_value, chatbot_value, e: gr.EventData):\n",
        "        index = e._data[\"payload\"][0][\"index\"]\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        history[index][\"content\"] = chatbot_value[index][\"content\"]\n",
        "        return gr.update(value=state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def regenerate_message(settings_form_value, thinking_btn_state_value,\n",
        "                           state_value, e: gr.EventData):\n",
        "        index = e._data[\"payload\"][0][\"index\"]\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "        history = history[:index]\n",
        "\n",
        "        state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]] = {\n",
        "                \"history\": history,\n",
        "                \"settings\": settings_form_value,\n",
        "                \"enable_thinking\": thinking_btn_state_value[\"enable_thinking\"]\n",
        "            }\n",
        "\n",
        "        yield Gradio_Events.preprocess_submit()(state_value)\n",
        "        try:\n",
        "            for chunk in Gradio_Events.submit(state_value):\n",
        "                yield chunk\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            yield Gradio_Events.postprocess_submit(state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def select_suggestion(input_value, e: gr.EventData):\n",
        "        input_value = input_value[:-1] + e._data[\"payload\"][0]\n",
        "        return gr.update(value=input_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_prompt(e: gr.EventData):\n",
        "        return gr.update(value=e._data[\"payload\"][0][\"value\"][\"description\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def new_chat(thinking_btn_state, state_value):\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            return gr.skip()\n",
        "        state_value[\"conversation_id\"] = \"\"\n",
        "        thinking_btn_state[\"enable_thinking\"] = True\n",
        "        return gr.update(active_key=state_value[\"conversation_id\"]), gr.update(\n",
        "            value=None), gr.update(value=DEFAULT_SETTINGS), gr.update(\n",
        "                value=thinking_btn_state), gr.update(value=state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def select_conversation(thinking_btn_state_value, state_value,\n",
        "                            e: gr.EventData):\n",
        "        active_key = e._data[\"payload\"][0]\n",
        "        if state_value[\"conversation_id\"] == active_key or (\n",
        "                active_key not in state_value[\"conversation_contexts\"]):\n",
        "            return gr.skip()\n",
        "        state_value[\"conversation_id\"] = active_key\n",
        "        thinking_btn_state_value[\"enable_thinking\"] = state_value[\n",
        "            \"conversation_contexts\"][active_key][\"enable_thinking\"]\n",
        "        return gr.update(active_key=active_key), gr.update(\n",
        "            value=state_value[\"conversation_contexts\"][active_key][\"history\"]\n",
        "        ), gr.update(value=state_value[\"conversation_contexts\"][active_key]\n",
        "                     [\"settings\"]), gr.update(\n",
        "                         value=thinking_btn_state_value), gr.update(\n",
        "                             value=state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def click_conversation_menu(state_value, e: gr.EventData):\n",
        "        conversation_id = e._data[\"payload\"][0][\"key\"]\n",
        "        operation = e._data[\"payload\"][1][\"key\"]\n",
        "        if operation == \"delete\":\n",
        "            del state_value[\"conversation_contexts\"][conversation_id]\n",
        "\n",
        "            state_value[\"conversations\"] = [\n",
        "                item for item in state_value[\"conversations\"]\n",
        "                if item[\"key\"] != conversation_id\n",
        "            ]\n",
        "\n",
        "            if state_value[\"conversation_id\"] == conversation_id:\n",
        "                state_value[\"conversation_id\"] = \"\"\n",
        "                return gr.update(\n",
        "                    items=state_value[\"conversations\"],\n",
        "                    active_key=state_value[\"conversation_id\"]), gr.update(\n",
        "                        value=None), gr.update(value=state_value)\n",
        "            else:\n",
        "                return gr.update(\n",
        "                    items=state_value[\"conversations\"]), gr.skip(), gr.update(\n",
        "                        value=state_value)\n",
        "        return gr.skip()\n",
        "\n",
        "    @staticmethod\n",
        "    def toggle_settings_header(settings_header_state_value):\n",
        "        settings_header_state_value[\n",
        "            \"open\"] = not settings_header_state_value[\"open\"]\n",
        "        return gr.update(value=settings_header_state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_conversation_history(state_value):\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            return gr.skip()\n",
        "        state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"] = []\n",
        "        return gr.update(value=None), gr.update(value=state_value)\n",
        "\n",
        "    @staticmethod\n",
        "    def update_browser_state(state_value):\n",
        "\n",
        "        return gr.update(value=dict(\n",
        "            conversations=state_value[\"conversations\"],\n",
        "            conversation_contexts=state_value[\"conversation_contexts\"]))\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_browser_state(browser_state_value, state_value):\n",
        "        state_value[\"conversations\"] = browser_state_value[\"conversations\"]\n",
        "        state_value[\"conversation_contexts\"] = browser_state_value[\n",
        "            \"conversation_contexts\"]\n",
        "        return gr.update(\n",
        "            items=browser_state_value[\"conversations\"]), gr.update(\n",
        "                value=state_value)\n",
        "\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "  padding: 0 !important;\n",
        "}\n",
        ".gradio-container > main.fillable {\n",
        "  padding: 0 !important;\n",
        "}\n",
        "#chatbot {\n",
        "  height: calc(100vh - 21px - 16px);\n",
        "  max-height: 1500px;\n",
        "}\n",
        "#chatbot .chatbot-conversations {\n",
        "  height: 100vh;\n",
        "  background-color: var(--ms-gr-ant-color-bg-layout);\n",
        "  padding-left: 4px;\n",
        "  padding-right: 4px;\n",
        "}\n",
        "#chatbot .chatbot-conversations .chatbot-conversations-list {\n",
        "  padding-left: 0;\n",
        "  padding-right: 0;\n",
        "}\n",
        "#chatbot .chatbot-chat {\n",
        "  padding: 32px;\n",
        "  padding-bottom: 0;\n",
        "  height: 100%;\n",
        "}\n",
        "@media (max-width: 768px) {\n",
        "  #chatbot .chatbot-chat {\n",
        "      padding: 0;\n",
        "  }\n",
        "}\n",
        "#chatbot .chatbot-chat .chatbot-chat-messages {\n",
        "  flex: 1;\n",
        "}\n",
        "#chatbot .setting-form-thinking-budget .ms-gr-ant-form-item-control-input-content {\n",
        "    display: flex;\n",
        "    flex-wrap: wrap;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model_options_map_json = json.dumps(MODEL_OPTIONS_MAP)\n",
        "js = \"function init() { window.MODEL_OPTIONS_MAP=\" + model_options_map_json + \"}\"\n",
        "\n",
        "with gr.Blocks(css=css, js=js, fill_width=True) as demo:\n",
        "    state = gr.State({\n",
        "        \"conversation_contexts\": {},\n",
        "        \"conversations\": [],\n",
        "        \"conversation_id\": \"\",\n",
        "    })\n",
        "\n",
        "    with ms.Application(), antdx.XProvider(\n",
        "            theme=DEFAULT_THEME, locale=DEFAULT_LOCALE), ms.AutoLoading():\n",
        "        with antd.Row(gutter=[20, 20], wrap=False, elem_id=\"chatbot\"):\n",
        "            # Left Column\n",
        "            with antd.Col(md=dict(flex=\"0 0 260px\", span=24, order=0),\n",
        "                          span=0,\n",
        "                          order=1):\n",
        "                with ms.Div(elem_classes=\"chatbot-conversations\"):\n",
        "                    with antd.Flex(vertical=True,\n",
        "                                   gap=\"small\",\n",
        "                                   elem_style=dict(height=\"100%\")):\n",
        "                        # Logo\n",
        "                        Logo()\n",
        "\n",
        "                        # New Conversation Button\n",
        "                        with antd.Button(value=None,\n",
        "                                         color=\"primary\",\n",
        "                                         variant=\"filled\",\n",
        "                                         block=True) as add_conversation_btn:\n",
        "                            ms.Text(get_text(\"New Conversation\", \"新建对话\"))\n",
        "                            with ms.Slot(\"icon\"):\n",
        "                                antd.Icon(\"PlusOutlined\")\n",
        "\n",
        "                        # Conversations List\n",
        "                        with antdx.Conversations(\n",
        "                                elem_classes=\"chatbot-conversations-list\",\n",
        "                        ) as conversations:\n",
        "                            with ms.Slot('menu.items'):\n",
        "                                with antd.Menu.Item(\n",
        "                                        label=\"Delete\", key=\"delete\",\n",
        "                                        danger=True\n",
        "                                ) as conversation_delete_menu_item:\n",
        "                                    with ms.Slot(\"icon\"):\n",
        "                                        antd.Icon(\"DeleteOutlined\")\n",
        "            # Right Column\n",
        "            with antd.Col(flex=1, elem_style=dict(height=\"100%\")):\n",
        "                with antd.Flex(vertical=True,\n",
        "                               gap=\"small\",\n",
        "                               elem_classes=\"chatbot-chat\"):\n",
        "                    # Chatbot\n",
        "                    chatbot = pro.Chatbot(elem_classes=\"chatbot-chat-messages\",\n",
        "                                          height=0,\n",
        "                                          welcome_config=welcome_config(),\n",
        "                                          user_config=user_config(),\n",
        "                                          bot_config=bot_config())\n",
        "\n",
        "                    # Input\n",
        "                    with antdx.Suggestion(\n",
        "                            items=DEFAULT_SUGGESTIONS,\n",
        "                            # onKeyDown Handler in Javascript\n",
        "                            should_trigger=\"\"\"(e, { onTrigger, onKeyDown }) => {\n",
        "                      switch(e.key) {\n",
        "                        case '/':\n",
        "                          onTrigger()\n",
        "                          break\n",
        "                        case 'ArrowRight':\n",
        "                        case 'ArrowLeft':\n",
        "                        case 'ArrowUp':\n",
        "                        case 'ArrowDown':\n",
        "                          break;\n",
        "                        default:\n",
        "                          onTrigger(false)\n",
        "                      }\n",
        "                      onKeyDown(e)\n",
        "                    }\"\"\") as suggestion:\n",
        "                        with ms.Slot(\"children\"):\n",
        "                            with antdx.Sender(placeholder=get_text(\n",
        "                                    \"Enter \\\"/\\\" to get suggestions\",\n",
        "                                    \"输入 \\\"/\\\" 获取提示\"), ) as input:\n",
        "                                with ms.Slot(\"header\"):\n",
        "                                    settings_header_state, settings_form = SettingsHeader(\n",
        "                                    )\n",
        "                                with ms.Slot(\"prefix\"):\n",
        "                                    with antd.Flex(\n",
        "                                            gap=4,\n",
        "                                            wrap=True,\n",
        "                                            elem_style=dict(maxWidth='40vw')):\n",
        "                                        with antd.Button(\n",
        "                                                value=None,\n",
        "                                                type=\"text\") as setting_btn:\n",
        "                                            with ms.Slot(\"icon\"):\n",
        "                                                antd.Icon(\"SettingOutlined\")\n",
        "                                        with antd.Button(\n",
        "                                                value=None,\n",
        "                                                type=\"text\") as clear_btn:\n",
        "                                            with ms.Slot(\"icon\"):\n",
        "                                                antd.Icon(\"ClearOutlined\")\n",
        "                                        thinking_btn_state = ThinkingButton()\n",
        "\n",
        "    # Events Handler\n",
        "    # Browser State Handler\n",
        "    if save_history:\n",
        "        browser_state = gr.BrowserState(\n",
        "            {\n",
        "                \"conversation_contexts\": {},\n",
        "                \"conversations\": [],\n",
        "            },\n",
        "            storage_key=\"qwen3_chat_demo_storage\")\n",
        "        state.change(fn=Gradio_Events.update_browser_state,\n",
        "                     inputs=[state],\n",
        "                     outputs=[browser_state])\n",
        "\n",
        "        demo.load(fn=Gradio_Events.apply_browser_state,\n",
        "                  inputs=[browser_state, state],\n",
        "                  outputs=[conversations, state])\n",
        "\n",
        "    # Conversations Handler\n",
        "    add_conversation_btn.click(fn=Gradio_Events.new_chat,\n",
        "                               inputs=[thinking_btn_state, state],\n",
        "                               outputs=[\n",
        "                                   conversations, chatbot, settings_form,\n",
        "                                   thinking_btn_state, state\n",
        "                               ])\n",
        "    conversations.active_change(fn=Gradio_Events.select_conversation,\n",
        "                                inputs=[thinking_btn_state, state],\n",
        "                                outputs=[\n",
        "                                    conversations, chatbot, settings_form,\n",
        "                                    thinking_btn_state, state\n",
        "                                ])\n",
        "    conversations.menu_click(fn=Gradio_Events.click_conversation_menu,\n",
        "                             inputs=[state],\n",
        "                             outputs=[conversations, chatbot, state])\n",
        "    # Chatbot Handler\n",
        "    chatbot.welcome_prompt_select(fn=Gradio_Events.apply_prompt,\n",
        "                                  outputs=[input])\n",
        "\n",
        "    chatbot.delete(fn=Gradio_Events.delete_message,\n",
        "                   inputs=[state],\n",
        "                   outputs=[state])\n",
        "    chatbot.edit(fn=Gradio_Events.edit_message,\n",
        "                 inputs=[state, chatbot],\n",
        "                 outputs=[state])\n",
        "\n",
        "    regenerating_event = chatbot.retry(\n",
        "        fn=Gradio_Events.regenerate_message,\n",
        "        inputs=[settings_form, thinking_btn_state, state],\n",
        "        outputs=[\n",
        "            input, clear_btn, conversation_delete_menu_item,\n",
        "            add_conversation_btn, conversations, chatbot, state\n",
        "        ])\n",
        "\n",
        "    # Input Handler\n",
        "    submit_event = input.submit(\n",
        "        fn=Gradio_Events.add_message,\n",
        "        inputs=[input, settings_form, thinking_btn_state, state],\n",
        "        outputs=[\n",
        "            input, clear_btn, conversation_delete_menu_item,\n",
        "            add_conversation_btn, conversations, chatbot, state\n",
        "        ])\n",
        "    input.cancel(fn=Gradio_Events.cancel,\n",
        "                 inputs=[state],\n",
        "                 outputs=[\n",
        "                     input, conversation_delete_menu_item, clear_btn,\n",
        "                     conversations, add_conversation_btn, chatbot, state\n",
        "                 ],\n",
        "                 cancels=[submit_event, regenerating_event],\n",
        "                 queue=False)\n",
        "    # Input Actions Handler\n",
        "    setting_btn.click(fn=Gradio_Events.toggle_settings_header,\n",
        "                      inputs=[settings_header_state],\n",
        "                      outputs=[settings_header_state])\n",
        "    clear_btn.click(fn=Gradio_Events.clear_conversation_history,\n",
        "                    inputs=[state],\n",
        "                    outputs=[chatbot, state])\n",
        "    suggestion.select(fn=Gradio_Events.select_suggestion,\n",
        "                      inputs=[input],\n",
        "                      outputs=[input])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(default_concurrency_limit=100,\n",
        "               max_size=100).launch(ssr_mode=False, max_threads=100)"
      ],
      "metadata": {
        "id": "NjySJ9tA_H4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import modelscope_studio.components.antd as antd\n",
        "import modelscope_studio.components.antdx as antdx\n",
        "import modelscope_studio.components.base as ms\n",
        "import modelscope_studio.components.pro as pro\n",
        "from config import DEFAULT_LOCALE, DEFAULT_THEME, get_text, user_config, bot_config, welcome_config\n",
        "from ui_components.logo import Logo\n",
        "from ui_components.settings_header import SettingsHeader\n",
        "\n",
        "# Initialize TinyLlama pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "class Gradio_Events:\n",
        "    @staticmethod\n",
        "    def generate_response(prompt):\n",
        "        \"\"\"Generate both reasoning and final response using TinyLlama\"\"\"\n",
        "        try:\n",
        "            # Generate reasoning\n",
        "            reasoning = pipe(\n",
        "                f\"Reflect on all potential nuances and concepts for: {prompt}\",\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "\n",
        "            # Generate final answer\n",
        "            response = pipe(\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "\n",
        "            return reasoning, response\n",
        "        except Exception as e:\n",
        "            return None, str(e)\n",
        "\n",
        "    @staticmethod\n",
        "    def submit(state_value):\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "\n",
        "        try:\n",
        "            user_input = history[-1][\"content\"]\n",
        "            start_time = time.time()\n",
        "            reasoning, answer = Gradio_Events.generate_response(user_input)\n",
        "\n",
        "            if not answer:\n",
        "                raise Exception(\"Failed to generate response\")\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            assistant_message = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"tool\",\n",
        "                        \"content\": reasoning,\n",
        "                        \"options\": {\n",
        "                            \"title\": get_text(\"Chain of Thought\", \"思考链\"),\n",
        "                            \"status\": \"done\"\n",
        "                        },\n",
        "                        \"copyable\": False,\n",
        "                        \"editable\": False\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"content\": f\"{answer}\\n\\n*({elapsed:.2f}s)*\",\n",
        "                    }\n",
        "                ],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            }\n",
        "\n",
        "            history.append(assistant_message)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\n",
        "                    \"type\": \"text\",\n",
        "                    \"content\": f'<span style=\"color: #e53935\">Error: {str(e)}</span>'\n",
        "                }],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            }\n",
        "            history.append(error_message)\n",
        "\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=history),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def add_message(input_value, state_value):\n",
        "        if not input_value.strip():\n",
        "            return gr.skip()\n",
        "\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            random_id = str(uuid.uuid4())\n",
        "            state_value[\"conversation_id\"] = random_id\n",
        "            state_value[\"conversation_contexts\"][random_id] = {\"history\": []}\n",
        "            state_value[\"conversations\"].append({\n",
        "                \"label\": input_value[:20] + (\"...\" if len(input_value) > 20 else \"\"),\n",
        "                \"key\": random_id\n",
        "            })\n",
        "\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "\n",
        "        history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": input_value,\n",
        "            \"key\": str(uuid.uuid4())\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"input\": gr.update(value=\"\"),\n",
        "            \"chatbot\": gr.update(value=history),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def new_chat(state_value):\n",
        "        state_value[\"conversation_id\"] = \"\"\n",
        "        return {\n",
        "            \"conversations\": gr.update(items=state_value[\"conversations\"]),\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_history(state_value):\n",
        "        if state_value[\"conversation_id\"]:\n",
        "            state_value[\"conversation_contexts\"][\n",
        "                state_value[\"conversation_id\"]][\"history\"] = []\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "# Custom CSS with red/blue/black theme\n",
        "css = \"\"\"\n",
        ":root {\n",
        "    --color-red: #ff4444;\n",
        "    --color-blue: #1e88e5;\n",
        "    --color-black: #000000;\n",
        "    --color-dark-gray: #121212;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    background: var(--color-black) !important;\n",
        "    color: white !important;\n",
        "}\n",
        "\n",
        "/* Input styling */\n",
        ".gr-textbox textarea {\n",
        "    background: var(--color-dark-gray) !important;\n",
        "    border: 2px solid var(--color-blue) !important;\n",
        "    color: white !important;\n",
        "}\n",
        "\n",
        "/* Output (chatbot) styling */\n",
        ".gr-chatbot {\n",
        "    background: var(--color-dark-gray) !important;\n",
        "    border: 2px solid var(--color-red) !important;\n",
        "}\n",
        "\n",
        "/* User message bubbles */\n",
        ".gr-chatbot .user {\n",
        "    background: var(--color-blue) !important;\n",
        "    border-color: var(--color-blue) !important;\n",
        "}\n",
        "\n",
        "/* Assistant message bubbles */\n",
        ".gr-chatbot .bot {\n",
        "    background: var(--color-dark-gray) !important;\n",
        "    border: 1px solid var(--color-red) !important;\n",
        "}\n",
        "\n",
        "/* Buttons */\n",
        ".gr-button {\n",
        "    background: var(--color-blue) !important;\n",
        "    border-color: var(--color-blue) !important;\n",
        "}\n",
        "\n",
        "/* Thinking tooltip */\n",
        ".gr-chatbot .tool {\n",
        "    background: var(--color-dark-gray) !important;\n",
        "    border: 1px solid var(--color-red) !important;\n",
        "}\n",
        "\"\"\"\n",
        "class Gradio_Events:\n",
        "    # Add state management for generation control\n",
        "    _generating = False\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_response(prompt):\n",
        "        \"\"\"Generate both reasoning and final response using TinyLlama\"\"\"\n",
        "        try:\n",
        "            # Generate reasoning\n",
        "            reasoning = pipe(\n",
        "                f\"Reflect on all potential nuances and concepts for: {prompt}\",\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "\n",
        "            # Generate final answer\n",
        "            response = pipe(\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "\n",
        "            return reasoning, response\n",
        "        except Exception as e:\n",
        "            return None, str(e)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_message(input_value, state_value):\n",
        "        if not input_value.strip():\n",
        "            return gr.skip()\n",
        "\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            random_id = str(uuid.uuid4())\n",
        "            state_value[\"conversation_id\"] = random_id\n",
        "            state_value[\"conversation_contexts\"][random_id] = {\"history\": []}\n",
        "            state_value[\"conversations\"].append({\n",
        "                \"label\": input_value[:20] + (\"...\" if len(input_value) > 20 else \"\"),\n",
        "                \"key\": random_id\n",
        "            })\n",
        "\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "\n",
        "        history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": input_value,\n",
        "            \"key\": str(uuid.uuid4())\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"input\": gr.update(value=\"\"),\n",
        "            \"chatbot\": gr.update(value=history),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def submit(state_value):\n",
        "        if Gradio_Events._generating:\n",
        "            return gr.skip()\n",
        "\n",
        "        Gradio_Events._generating = True\n",
        "        history = state_value[\"conversation_contexts\"][\n",
        "            state_value[\"conversation_id\"]][\"history\"]\n",
        "\n",
        "        try:\n",
        "            user_input = history[-1][\"content\"]\n",
        "            start_time = time.time()\n",
        "            reasoning, answer = Gradio_Events.generate_response(user_input)\n",
        "\n",
        "            if not answer:\n",
        "                raise Exception(\"Failed to generate response\")\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            assistant_message = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"tool\",\n",
        "                        \"content\": reasoning,\n",
        "                        \"options\": {\n",
        "                            \"title\": get_text(\"Chain of Thought\", \"思考链\"),\n",
        "                            \"status\": \"done\"\n",
        "                        },\n",
        "                        \"copyable\": False,\n",
        "                        \"editable\": False\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"content\": f\"{answer}\\n\\n*({elapsed:.2f}s)*\",\n",
        "                        \"style\": {\"color\": \"#ff4444\"}\n",
        "                    }\n",
        "                ],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            }\n",
        "\n",
        "            history.append(assistant_message)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\n",
        "                    \"type\": \"text\",\n",
        "                    \"content\": f'<span style=\"color: #ff4444\">Error: {str(e)}</span>'\n",
        "                }],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            }\n",
        "            history.append(error_message)\n",
        "\n",
        "        Gradio_Events._generating = False\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=history),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def new_chat(state_value):\n",
        "        state_value[\"conversation_id\"] = \"\"\n",
        "        return {\n",
        "            \"conversations\": gr.update(items=state_value[\"conversations\"]),\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_history(state_value):\n",
        "        if state_value[\"conversation_id\"]:\n",
        "            state_value[\"conversation_contexts\"][\n",
        "                state_value[\"conversation_id\"]][\"history\"] = []\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "with gr.Blocks(css=css, fill_width=True) as demo:\n",
        "    state = gr.State({\n",
        "        \"conversation_contexts\": {},\n",
        "        \"conversations\": [],\n",
        "        \"conversation_id\": \"\",\n",
        "    })\n",
        "\n",
        "    with ms.Application(), antdx.XProvider(\n",
        "            theme=DEFAULT_THEME, locale=DEFAULT_LOCALE), ms.AutoLoading():\n",
        "        with antd.Row(gutter=[20, 20], wrap=False, elem_id=\"chatbot\"):\n",
        "            # Left Column\n",
        "            with antd.Col(md=dict(flex=\"0 0 260px\", span=24, order=0),\n",
        "                        span=0, order=1):\n",
        "                with ms.Div(elem_classes=\"chatbot-conversations\"):\n",
        "                    with antd.Flex(vertical=True, gap=\"small\",\n",
        "                                  elem_style=dict(height=\"100%\")):\n",
        "                        Logo()\n",
        "\n",
        "                        # New Chat Button\n",
        "                        with antd.Button(\n",
        "                            color=\"primary\",\n",
        "                            variant=\"filled\",\n",
        "                            block=True,\n",
        "                            elem_classes=\"new-chat-btn\"\n",
        "                        ) as new_chat_btn:\n",
        "                            ms.Text(get_text(\"New Chat\", \"新建对话\"))\n",
        "                            with ms.Slot(\"icon\"):\n",
        "                                antd.Icon(\"PlusOutlined\")\n",
        "\n",
        "                        # Conversations List\n",
        "                        with antdx.Conversations(\n",
        "                                elem_classes=\"chatbot-conversations-list\"\n",
        "                        ) as conversations:\n",
        "                            with ms.Slot('menu.items'):\n",
        "                                with antd.Menu.Item(\n",
        "                                        label=\"Delete\",\n",
        "                                        key=\"delete\",\n",
        "                                        danger=True\n",
        "                                ) as conversation_delete_menu_item:\n",
        "                                    with ms.Slot(\"icon\"):\n",
        "                                        antd.Icon(\"DeleteOutlined\")\n",
        "\n",
        "            # Right Column\n",
        "            with antd.Col(flex=1, elem_style=dict(height=\"100%\")):\n",
        "                with antd.Flex(vertical=True, gap=\"small\",\n",
        "                               elem_classes=\"chatbot-chat\"):\n",
        "                    # Chat Display\n",
        "                    chatbot = pro.Chatbot(\n",
        "                        elem_classes=\"chatbot-chat-messages\",\n",
        "                        height=600,\n",
        "                        welcome_config=welcome_config(),\n",
        "                        user_config=user_config(),\n",
        "                        bot_config=bot_config()\n",
        "                    )\n",
        "\n",
        "                    # Input Area\n",
        "                    with antdx.Suggestion(items=[]):\n",
        "                        with ms.Slot(\"children\"):\n",
        "                            with antdx.Sender(\n",
        "                                placeholder=\"Type your message...\",\n",
        "                                elem_classes=\"chat-input\"\n",
        "                            ) as input:\n",
        "                                with ms.Slot(\"prefix\"):\n",
        "                                    with antd.Flex(gap=4):\n",
        "                                        with antd.Button(\n",
        "                                            type=\"text\",\n",
        "                                            elem_classes=\"clear-btn\"\n",
        "                                        ) as clear_btn:\n",
        "                                            with ms.Slot(\"icon\"):\n",
        "                                                antd.Icon(\"ClearOutlined\")\n",
        "\n",
        "    # Event Handlers\n",
        "    input.submit(\n",
        "        fn=Gradio_Events.add_message,\n",
        "        inputs=[input, state],\n",
        "        outputs=[input, chatbot, state]\n",
        "    ).then(\n",
        "        fn=Gradio_Events.submit,\n",
        "        inputs=[state],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "    new_chat_btn.click(\n",
        "        fn=Gradio_Events.new_chat,\n",
        "        inputs=[state],\n",
        "        outputs=[conversations, chatbot, state]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=Gradio_Events.clear_history,\n",
        "        inputs=[state],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "demo.queue().launch()"
      ],
      "metadata": {
        "id": "vB2lCxidKcRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import time\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import modelscope_studio.components.antd as antd\n",
        "import modelscope_studio.components.antdx as antdx\n",
        "import modelscope_studio.components.base as ms\n",
        "import modelscope_studio.components.pro as pro\n",
        "from config import (\n",
        "    DEFAULT_LOCALE, DEFAULT_THEME, get_text,\n",
        "    user_config, bot_config, welcome_config,\n",
        "    DEFAULT_SUGGESTIONS\n",
        ")\n",
        "from ui_components.logo import Logo\n",
        "from ui_components.settings_header import SettingsHeader\n",
        "\n",
        "# Initialize TinyLlama pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "class Gradio_Events:\n",
        "    _generating = False\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_response(prompt):\n",
        "        try:\n",
        "            reasoning = pipe(\n",
        "                f\"Reflect on all potential nuances and concepts for: {prompt}\",\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "            response = pipe(\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )[0][\"generated_text\"].strip()\n",
        "            return reasoning, response\n",
        "        except Exception as e:\n",
        "            return None, str(e)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_message(input_value, state_value):\n",
        "        if not input_value.strip():\n",
        "            return gr.skip()\n",
        "        # create new conversation if needed\n",
        "        if not state_value[\"conversation_id\"]:\n",
        "            cid = str(uuid.uuid4())\n",
        "            state_value[\"conversation_id\"] = cid\n",
        "            state_value[\"conversation_contexts\"][cid] = {\"history\": []}\n",
        "            state_value[\"conversations\"].append({\n",
        "                \"label\": \"New Chat\",\n",
        "                \"key\": cid\n",
        "            })\n",
        "        hist = state_value[\"conversation_contexts\"][state_value[\"conversation_id\"]][\"history\"]\n",
        "        hist.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": input_value,\n",
        "            \"key\": str(uuid.uuid4())\n",
        "        })\n",
        "        return {\n",
        "            \"input\": gr.update(value=\"\"),\n",
        "            \"chatbot\": gr.update(value=hist),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def submit(state_value):\n",
        "        if Gradio_Events._generating:\n",
        "            return gr.skip()\n",
        "        Gradio_Events._generating = True\n",
        "\n",
        "        hist = state_value[\"conversation_contexts\"][state_value[\"conversation_id\"]][\"history\"]\n",
        "        user_input = hist[-1][\"content\"]\n",
        "        start = time.time()\n",
        "        reasoning, answer = Gradio_Events.generate_response(user_input)\n",
        "        if answer is None:\n",
        "            hist.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\n",
        "                    \"type\": \"text\",\n",
        "                    \"content\": f'<span style=\"color:red\">Error generating response</span>'\n",
        "                }],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            })\n",
        "        else:\n",
        "            elapsed = time.time() - start\n",
        "            hist.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"tool\",\n",
        "                        \"content\": reasoning,\n",
        "                        \"options\": {\n",
        "                            \"title\": get_text(\"Chain of Thought\", \"思考链\"),\n",
        "                            \"status\": \"done\"\n",
        "                        },\n",
        "                        \"copyable\": False,\n",
        "                        \"editable\": False\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"content\": f\"{answer}\\n\\n*({elapsed:.2f}s)*\"\n",
        "                    }\n",
        "                ],\n",
        "                \"key\": str(uuid.uuid4()),\n",
        "                \"loading\": False,\n",
        "                \"status\": \"done\"\n",
        "            })\n",
        "\n",
        "        Gradio_Events._generating = False\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=hist),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def new_chat(state_value):\n",
        "        state_value[\"conversation_id\"] = \"\"\n",
        "        return {\n",
        "            \"conversations\": gr.update(items=state_value[\"conversations\"]),\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_history(state_value):\n",
        "        cid = state_value[\"conversation_id\"]\n",
        "        if cid:\n",
        "            state_value[\"conversation_contexts\"][cid][\"history\"] = []\n",
        "        return {\n",
        "            \"chatbot\": gr.update(value=[]),\n",
        "            \"state\": gr.update(value=state_value)\n",
        "        }\n",
        "\n",
        "\n",
        "# Custom CSS\n",
        "css = \"\"\"\n",
        ":root {\n",
        "  --color-red: #ff4444;\n",
        "  --color-blue: #1e88e5;\n",
        "  --color-black: #000000;\n",
        "  --color-dark: #121212;\n",
        "}\n",
        ".gradio-container {\n",
        "  background: var(--color-black) !important;\n",
        "  color: #fff !important;\n",
        "}\n",
        ".gr-textbox textarea {\n",
        "  background: var(--color-dark) !important;\n",
        "  border: 2px solid var(--color-blue) !important;\n",
        "  color: #fff !important;\n",
        "}\n",
        ".gr-chatbot {\n",
        "  background: var(--color-dark) !important;\n",
        "  border: 2px solid var(--color-red) !important;\n",
        "}\n",
        ".gr-chatbot .user {\n",
        "  background: var(--color-blue) !important;\n",
        "  border-color: var(--color-blue) !important;\n",
        "}\n",
        ".gr-chatbot .bot {\n",
        "  background: var(--color-dark) !important;\n",
        "  border: 1px solid var(--color-red) !important;\n",
        "}\n",
        ".gr-button {\n",
        "  background: var(--color-blue) !important;\n",
        "  border-color: var(--color-blue) !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with gr.Blocks(css=css, fill_width=True) as demo:\n",
        "    # Initialize an empty conversation\n",
        "    initial_id = str(uuid.uuid4())\n",
        "    state = gr.State({\n",
        "        \"conversation_contexts\": { initial_id: {\"history\": []} },\n",
        "        \"conversations\": [ {\"label\": \"New Chat\", \"key\": initial_id} ],\n",
        "        \"conversation_id\": initial_id\n",
        "    })\n",
        "\n",
        "    with ms.Application(), antdx.XProvider(theme=DEFAULT_THEME, locale=DEFAULT_LOCALE), ms.AutoLoading():\n",
        "        with antd.Row(gutter=[20, 20], wrap=False, elem_id=\"chatbot\"):\n",
        "            # Sidebar\n",
        "            with antd.Col(md=dict(flex=\"0 0 260px\", span=24), span=0):\n",
        "                with ms.Div(elem_classes=\"chatbot-conversations\"):\n",
        "                    with antd.Flex(vertical=True, gap=\"small\", elem_style={\"height\": \"100%\"}):\n",
        "                        Logo()\n",
        "                        with antd.Button(color=\"primary\", variant=\"filled\", block=True) as new_btn:\n",
        "                            ms.Text(get_text(\"New Chat\", \"新对话\"))\n",
        "                            with ms.Slot(\"icon\"):\n",
        "                                antd.Icon(\"PlusOutlined\")\n",
        "                        with antdx.Conversations(elem_classes=\"chatbot-conversations-list\") as convs:\n",
        "                            with ms.Slot(\"menu.items\"):\n",
        "                                with antd.Menu.Item(label=\"Delete\", key=\"delete\", danger=True) as del_item:\n",
        "                                    with ms.Slot(\"icon\"):\n",
        "                                        antd.Icon(\"DeleteOutlined\")\n",
        "\n",
        "            # Chat area\n",
        "            with antd.Col(flex=1, elem_style={\"height\": \"100%\"}):\n",
        "                with antd.Flex(vertical=True, gap=\"small\", elem_classes=\"chatbot-chat\"):\n",
        "                    # Pro Chatbot display\n",
        "                    chatbot = pro.Chatbot(\n",
        "                        elem_classes=\"chatbot-chat-messages\",\n",
        "                        height=600,\n",
        "                        welcome_config=welcome_config(),\n",
        "                        user_config=user_config(),\n",
        "                        bot_config=bot_config()\n",
        "                    )\n",
        "                    # Suggestion + Input\n",
        "                    with antdx.Suggestion(items=DEFAULT_SUGGESTIONS) as suggestion:\n",
        "                        with ms.Slot(\"children\"):\n",
        "                            with antdx.Sender(\n",
        "                                placeholder=get_text(\"Type message...\", \"输入消息...\"),\n",
        "                                elem_classes=\"chat-input\"\n",
        "                            ) as input_box:\n",
        "                                with ms.Slot(\"prefix\"):\n",
        "                                    with antd.Flex(gap=4):\n",
        "                                        with antd.Button(type=\"text\", elem_classes=\"clear-btn\") as clear_btn:\n",
        "                                            with ms.Slot(\"icon\"):\n",
        "                                                antd.Icon(\"ClearOutlined\")\n",
        "\n",
        "                    # Make default suggestions clickable\n",
        "                    suggestion.select(\n",
        "                        lambda val: gr.update(value=val),\n",
        "                        inputs=[],\n",
        "                        outputs=[input_box]\n",
        "                    )\n",
        "                    # Make welcome prompts clickable\n",
        "                    chatbot.welcome_prompt_select(\n",
        "                        lambda choice: gr.update(value=choice[\"description\"]),\n",
        "                        outputs=[input_box]\n",
        "                    )\n",
        "\n",
        "    # Event handlers\n",
        "    input_box.submit(\n",
        "        Gradio_Events.add_message, [input_box, state],\n",
        "        [input_box, chatbot, state]\n",
        "    ).then(\n",
        "        Gradio_Events.submit, [state],\n",
        "        [chatbot, state]\n",
        "    )\n",
        "    new_btn.click(\n",
        "        Gradio_Events.new_chat, [state],\n",
        "        [convs, chatbot, state]\n",
        "    )\n",
        "    clear_btn.click(\n",
        "        Gradio_Events.clear_history, [state],\n",
        "        [chatbot, state]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue().launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "8cEwPzU9nQju",
        "outputId": "81fc8ddb-6ef4-4cb7-b03f-774e6b169610"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1018: UserWarning: Expected 1 arguments for function <function <lambda> at 0x7f4f2e548400>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1022: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x7f4f2e548400>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1018: UserWarning: Expected 1 arguments for function <function <lambda> at 0x7f4f2e548c20>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1022: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x7f4f2e548c20>, received 0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eea847c1133292358b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eea847c1133292358b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}